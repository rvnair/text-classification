Resetting modules to system default. Reseting $MODULEPATH back to system default. All extra directories will be removed from $MODULEPATH.

Lmod is automatically replacing "python2/2.7.16" with "python3/3.7.0".

Epoch: 1 | Batch: 0/39999 (0%) | Loss: 0.709866
Epoch: 1 | Batch: 320/39999 (1%) | Loss: 0.723660
Epoch: 1 | Batch: 640/39999 (2%) | Loss: 0.812140
Epoch: 1 | Batch: 960/39999 (2%) | Loss: 0.696494
Epoch: 1 | Batch: 1280/39999 (3%) | Loss: 0.702044
Epoch: 1 | Batch: 1600/39999 (4%) | Loss: 0.659480
Epoch: 1 | Batch: 1920/39999 (5%) | Loss: 1.003781
Epoch: 1 | Batch: 2240/39999 (6%) | Loss: 0.796130
Epoch: 1 | Batch: 2560/39999 (6%) | Loss: 0.673316
Epoch: 1 | Batch: 2880/39999 (7%) | Loss: 0.700979
Epoch: 1 | Batch: 3200/39999 (8%) | Loss: 0.691474
Epoch: 1 | Batch: 3520/39999 (9%) | Loss: 0.693176
Epoch: 1 | Batch: 3840/39999 (10%) | Loss: 0.703991
Epoch: 1 | Batch: 4160/39999 (10%) | Loss: 0.667741
Epoch: 1 | Batch: 4480/39999 (11%) | Loss: 0.698969
Epoch: 1 | Batch: 4800/39999 (12%) | Loss: 0.706786
Epoch: 1 | Batch: 5120/39999 (13%) | Loss: 0.750829
Epoch: 1 | Batch: 5440/39999 (14%) | Loss: 0.820958
Epoch: 1 | Batch: 5760/39999 (14%) | Loss: 0.690433
Epoch: 1 | Batch: 6080/39999 (15%) | Loss: 0.683191
Epoch: 1 | Batch: 6400/39999 (16%) | Loss: 0.713894
Epoch: 1 | Batch: 6720/39999 (17%) | Loss: 0.726224
Epoch: 1 | Batch: 7040/39999 (18%) | Loss: 0.691431
Epoch: 1 | Batch: 7360/39999 (18%) | Loss: 0.701795
Epoch: 1 | Batch: 7680/39999 (19%) | Loss: 0.701842
Epoch: 1 | Batch: 8000/39999 (20%) | Loss: 0.687075
Epoch: 1 | Batch: 8320/39999 (21%) | Loss: 0.692957
Epoch: 1 | Batch: 8640/39999 (22%) | Loss: 0.693928
Epoch: 1 | Batch: 8960/39999 (22%) | Loss: 0.703034
Epoch: 1 | Batch: 9280/39999 (23%) | Loss: 0.697080
Epoch: 1 | Batch: 9600/39999 (24%) | Loss: 0.692145
Epoch: 1 | Batch: 9920/39999 (25%) | Loss: 0.700798
Epoch: 1 | Batch: 10240/39999 (26%) | Loss: 0.677314
Epoch: 1 | Batch: 10560/39999 (26%) | Loss: 0.693195
Epoch: 1 | Batch: 10880/39999 (27%) | Loss: 0.655197
Epoch: 1 | Batch: 11200/39999 (28%) | Loss: 0.698145
Epoch: 1 | Batch: 11520/39999 (29%) | Loss: 0.693746
Epoch: 1 | Batch: 11840/39999 (30%) | Loss: 0.713432
Epoch: 1 | Batch: 12160/39999 (30%) | Loss: 0.691361
Epoch: 1 | Batch: 12480/39999 (31%) | Loss: 0.688801
Epoch: 1 | Batch: 12800/39999 (32%) | Loss: 0.672612
Epoch: 1 | Batch: 13120/39999 (33%) | Loss: 0.702842
Epoch: 1 | Batch: 13440/39999 (34%) | Loss: 0.692309
Epoch: 1 | Batch: 13760/39999 (34%) | Loss: 0.689908
Epoch: 1 | Batch: 14080/39999 (35%) | Loss: 0.694723
Epoch: 1 | Batch: 14400/39999 (36%) | Loss: 0.691846
Epoch: 1 | Batch: 14720/39999 (37%) | Loss: 0.694715
Epoch: 1 | Batch: 15040/39999 (38%) | Loss: 0.683808
Epoch: 1 | Batch: 15360/39999 (38%) | Loss: 0.700354
Epoch: 1 | Batch: 15680/39999 (39%) | Loss: 0.708449
Epoch: 1 | Batch: 16000/39999 (40%) | Loss: 0.691250
Epoch: 1 | Batch: 16320/39999 (41%) | Loss: 0.685564
Epoch: 1 | Batch: 16640/39999 (42%) | Loss: 0.713485
Epoch: 1 | Batch: 16960/39999 (42%) | Loss: 0.693823
Epoch: 1 | Batch: 17280/39999 (43%) | Loss: 0.695475
Epoch: 1 | Batch: 17600/39999 (44%) | Loss: 0.761938
Epoch: 1 | Batch: 17920/39999 (45%) | Loss: 0.687135
Epoch: 1 | Batch: 18240/39999 (46%) | Loss: 0.690545
Epoch: 1 | Batch: 18560/39999 (46%) | Loss: 0.657378
Epoch: 1 | Batch: 18880/39999 (47%) | Loss: 0.675633
Epoch: 1 | Batch: 19200/39999 (48%) | Loss: 0.761716
Epoch: 1 | Batch: 19520/39999 (49%) | Loss: 0.710329
Epoch: 1 | Batch: 19840/39999 (50%) | Loss: 0.684829
Epoch: 1 | Batch: 20160/39999 (50%) | Loss: 0.686276
Epoch: 1 | Batch: 20480/39999 (51%) | Loss: 0.698486
Epoch: 1 | Batch: 20800/39999 (52%) | Loss: 0.710324
Epoch: 1 | Batch: 21120/39999 (53%) | Loss: 0.684855
Epoch: 1 | Batch: 21440/39999 (54%) | Loss: 0.681923
Epoch: 1 | Batch: 21760/39999 (54%) | Loss: 0.701730
Epoch: 1 | Batch: 22080/39999 (55%) | Loss: 0.682519
Epoch: 1 | Batch: 22400/39999 (56%) | Loss: 0.703729
Epoch: 1 | Batch: 22720/39999 (57%) | Loss: 0.698667
Epoch: 1 | Batch: 23040/39999 (58%) | Loss: 0.719965
Epoch: 1 | Batch: 23360/39999 (58%) | Loss: 0.694678
Epoch: 1 | Batch: 23680/39999 (59%) | Loss: 0.738548
Epoch: 1 | Batch: 24000/39999 (60%) | Loss: 0.738543
Epoch: 1 | Batch: 24320/39999 (61%) | Loss: 0.685759
Epoch: 1 | Batch: 24640/39999 (62%) | Loss: 0.747296
Epoch: 1 | Batch: 24960/39999 (62%) | Loss: 0.693977
Epoch: 1 | Batch: 25280/39999 (63%) | Loss: 0.703837
Epoch: 1 | Batch: 25600/39999 (64%) | Loss: 0.693947
Epoch: 1 | Batch: 25920/39999 (65%) | Loss: 0.732836
Epoch: 1 | Batch: 26240/39999 (66%) | Loss: 0.729511
Epoch: 1 | Batch: 26560/39999 (66%) | Loss: 0.721792
Epoch: 1 | Batch: 26880/39999 (67%) | Loss: 0.702882
Epoch: 1 | Batch: 27200/39999 (68%) | Loss: 0.712389
Epoch: 1 | Batch: 27520/39999 (69%) | Loss: 0.731435
Epoch: 1 | Batch: 27840/39999 (70%) | Loss: 0.678659
Epoch: 1 | Batch: 28160/39999 (70%) | Loss: 0.696468
Epoch: 1 | Batch: 28480/39999 (71%) | Loss: 0.691234
Epoch: 1 | Batch: 28800/39999 (72%) | Loss: 0.689541
Epoch: 1 | Batch: 29120/39999 (73%) | Loss: 0.687119
Epoch: 1 | Batch: 29440/39999 (74%) | Loss: 0.724668
Epoch: 1 | Batch: 29760/39999 (74%) | Loss: 0.696551
Epoch: 1 | Batch: 30080/39999 (75%) | Loss: 0.689875
Epoch: 1 | Batch: 30400/39999 (76%) | Loss: 0.734672
Epoch: 1 | Batch: 30720/39999 (77%) | Loss: 0.710970
Epoch: 1 | Batch: 31040/39999 (78%) | Loss: 0.640886
Epoch: 1 | Batch: 31360/39999 (78%) | Loss: 0.694191
Epoch: 1 | Batch: 31680/39999 (79%) | Loss: 0.694390
Epoch: 1 | Batch: 32000/39999 (80%) | Loss: 0.693747
Epoch: 1 | Batch: 32320/39999 (81%) | Loss: 0.690678
Epoch: 1 | Batch: 32640/39999 (82%) | Loss: 0.691387
Epoch: 1 | Batch: 32960/39999 (82%) | Loss: 0.660287
Epoch: 1 | Batch: 33280/39999 (83%) | Loss: 0.682385
Epoch: 1 | Batch: 33600/39999 (84%) | Loss: 0.698895
Epoch: 1 | Batch: 33920/39999 (85%) | Loss: 0.704011
Epoch: 1 | Batch: 34240/39999 (86%) | Loss: 0.691690
Epoch: 1 | Batch: 34560/39999 (86%) | Loss: 0.696456
Epoch: 1 | Batch: 34880/39999 (87%) | Loss: 0.692694
Epoch: 1 | Batch: 35200/39999 (88%) | Loss: 0.713982
Epoch: 1 | Batch: 35520/39999 (89%) | Loss: 0.727553
Epoch: 1 | Batch: 35840/39999 (90%) | Loss: 0.700209
Epoch: 1 | Batch: 36160/39999 (90%) | Loss: 0.689781
Epoch: 1 | Batch: 36480/39999 (91%) | Loss: 0.684847
Epoch: 1 | Batch: 36800/39999 (92%) | Loss: 0.714969
Epoch: 1 | Batch: 37120/39999 (93%) | Loss: 0.688204
Epoch: 1 | Batch: 37440/39999 (94%) | Loss: 0.686976
Epoch: 1 | Batch: 37760/39999 (94%) | Loss: 0.677781
Epoch: 1 | Batch: 38080/39999 (95%) | Loss: 0.687154
Epoch: 1 | Batch: 38400/39999 (96%) | Loss: 0.696677
Epoch: 1 | Batch: 38720/39999 (97%) | Loss: 0.696183
Epoch: 1 | Batch: 39040/39999 (98%) | Loss: 0.694040
Epoch: 1 | Batch: 39360/39999 (98%) | Loss: 0.698308
Epoch: 1 | Batch: 39680/39999 (99%) | Loss: 0.696787
* (Train) Epoch: 1 | Loss: 0.7048
Traceback (most recent call last):
  File "code/berttrain.py", line 116, in <module>
    loss = train(epoch, device)
  File "code/berttrain.py", line 43, in train
    return validate_model(device, epoch)
  File "code/berttrain.py", line 54, in validate_model
    validLoss += loss.item()
ValueError: only one element tensors can be converted to Python scalars
